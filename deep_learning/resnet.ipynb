{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet (Residual Neural Network)\n",
    "\n",
    "Very, very deep neural networks are difficult to train, because of vanishing and exploding gradient types of problems. RestNet help skip connections which allows you to take the activation from one layer and suddenly feed it to another layer even much deeper in the neural network. And using that, you'll build ResNet which enables you to train very, very deep networks. Sometimes even networks of over 100 layers.\n",
    "\n",
    "## I. Residual block \n",
    "\n",
    "Thông thường, ta sẽ có mô hình như sau trong Neural network\n",
    "\n",
    "![](../img/resnet_1.png)\n",
    "\n",
    "Với mô hình trên, ta sẽ đi lần lượt từ `previous layer output -> linear process -> ReLu -> next layer input`\n",
    "\n",
    "=> Đối với khái niệm Residual block, ta có thể mang kết quả output của một layer trước đó để thực hiện phép cộng với ouput trước khi qua activate function là ReLu của layer phía sau nó như sau:\n",
    "\n",
    "![](../img/resnet_2.png)\n",
    "\n",
    "Việc thực hiện như mô hình trên còn có tên gọi `short cut/skip connection`\n",
    "\n",
    "## II. Residual Network\n",
    "\n",
    "Trong paper [He et al.,2015. Deep residual networks for image recognition], đối với mô hình neural network thông thường sau đây được gọi là `plain network`\n",
    "\n",
    "![](../img/resnet_3.png)\n",
    "\n",
    "Residual Netword là hệ thống các Residual block nối tiếp nhau => Từ mô hình `plain network` ta có `residual network` như sau:\n",
    "\n",
    "![](../img/resnet_4.png)\n",
    "\n",
    "## III. Tại sao phải sử dụng ResNet và tại sao ResNet lại có thể làm được như vậy\n",
    "\n",
    "### 1. Tại sao nên sử dụng ResNet\n",
    "\n",
    "- Với bài toán đặt ra là tăng layer của Network => Theo lý thuyết thì ta sẽ giúp làm hạn chế high bias(under fitting), loss sẽ giảm đi nhưng thực tế thì cho thấy `train loss` sẽ giảm đến một mức nhất định khi tăng số layers của network lên, sau đó sẽ lại tăng lên, biểu thị ở đồ thị bên dưới => Không tốt.\n",
    "\n",
    "- Sử dụng `ResNet` cho thấy đáp ứng được với yêu cầu đặt ra là càng nhiều layer => `train loss` càng giảm\n",
    "\n",
    "![](../img/resnet_5.png)\n",
    "\n",
    "### 2. Tại sao ResNet có thể làm được như thế\n",
    "\n",
    "## IV. So sánh mô hình\n",
    "\n",
    "![](../img/resnet_6.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2fce2166ecf35e10e9706810c05663b0d742ad8d6d30f48ef9e954a1f466467"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
